# -*- coding: utf-8 -*-
"""SVD  on Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gDhBBHMG7CugmOpxxslJRdYGBmoFVXVg
"""

sentences  = []
inputMediumfile = open("/content/CLIMATE_CHANGE1.txt")
for sentence in inputMediumfile:
  sentences.append(sentence)
print(sentences[10])

import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer

vec = TfidfVectorizer()
tf_idf =  vec.fit_transform(sentences)

print(pd.DataFrame(tf_idf.toarray() ))

import gensim

# Load Google's pre-trained Word2Vec model.
model =  gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True)

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

row = len(sentences)
col = len(vector2) 
sentenceMatrix = np.zeros((row, col),dtype=float)


i = 0
stopWords = set(stopwords.words('english'))
wordsFiltered = []

for i in range(row):
  words = word_tokenize(sentences[i])
  for w in words:
    if w not in stopWords:
      wordsFiltered.append(w)

  vector_total = np.zeros(col)
  for word_ in wordsFiltered:
    try:
      print(word_)
      vector1= model[word_]  
      vector_total = vector1 + vector_total
      #print(vector1)
    except KeyError:
      print("word '%s' not in vocabulary" % word_)
    sentenceMatrix[i,:] = vector_total


print("sentenceMatrix is  ", sentenceMatrix[3])

dist_1 = spatial.distance.cosine(sentenceMatrix[1], sentenceMatrix[10])
print('dist_1 ',dist_1)

u,s,vh = np.linalg.svd(sentenceMatrix)

len(u)

u.shape , vh.shape, s.shape

sen1 = u[:,:2]
vector1= sen1[1]
10 * vector1

sen2 = u[:,:2]
vector2 = sen2[10]
10 * vector2

u[1,:]

u[10,:]

dist_1 = spatial.distance.cosine(u[1,:], u[10,:])
print('dist_1 ',dist_1)

dist_1 = spatial.distance.cosine(u[1,:], u[10,:])
print('dist_1 ',dist_1)

dist_1 = spatial.distance.cosine(vector1[:2], vector2[:2])
print('dist_1 ',dist_1)

"""# Loading word2vec"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np

row = len(sentences)
col = len(vector2) 
sentenceMatrix = np.zeros((row, col),dtype=int)

sentenceMatrix

"""finding the repreentation of sentences in form of word vectors

## making sentence to concept mapping
"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords

stops = set(stopwords.words('english'))
print(stops)

"""## Converting sentences to average of word representation"""

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

row = len(sentences)
col = len(vector2) 
sentenceMatrix = np.zeros((row, col),dtype=float)


i = 1
stopWords = set(stopwords.words('english'))
wordsFiltered = []

words = word_tokenize(sentences[i])
for w in words:
  if w not in stopWords:
    wordsFiltered.append(w)

vector_total = np.zeros(col)
for word_ in wordsFiltered:
#  if (len(model[word_]) != 0): 
    try:
      print(word_)
      vector1= model[word_]  
      vector_total = vector1 + vector_total
      #print(vector1)
    except KeyError:
      print("word '%s' not in vocabulary" % word_)
    sentenceMatrix[i,:] = vector_total


print("sentenceMatrix is  ", sentenceMatrix[i])

"""## For all sentences, making the matrix to reduce"""

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

row = len(sentences)
col = len(vector2) 
sentenceMatrix = np.zeros((row, col),dtype=float)


i = 0
stopWords = set(stopwords.words('english'))
wordsFiltered = []

for i in range(row):
  words = word_tokenize(sentences[i])
  for w in words:
    if w not in stopWords:
      wordsFiltered.append(w)

  vector_total = np.zeros(col)
  for word_ in wordsFiltered:
    try:
      print(word_)
      vector1= model[word_]  
      vector_total = vector1 + vector_total
      #print(vector1)
    except KeyError:
      print("word '%s' not in vocabulary" % word_)
    sentenceMatrix[i,:] = vector_total


print("sentenceMatrix is  ", sentenceMatrix[3])